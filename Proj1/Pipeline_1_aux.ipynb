{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dlc_practical_prologue import generate_pair_sets\n",
    "import torch.nn as nn\n",
    "from print_util import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate raw data, process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_device(n, device='cpu'):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(n)\n",
    "    train_input = train_input.to(device=device)\n",
    "    train_target = train_target.to(device=device)\n",
    "    train_classes = train_classes.to(device=device)\n",
    "    test_input = test_input.to(device=device)\n",
    "    test_target = test_target.to(device=device)\n",
    "    test_classes = test_classes.to(device=device)\n",
    "    return train_input, train_target, train_classes, test_input, test_target, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_data_device(1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(tensor):\n",
    "    mu, std = tensor.mean(), tensor.std()\n",
    "    tmp = tensor.sub(mu).div(std)\n",
    "\n",
    "#     mu, std = tensor.mean(0), tensor.std(0)\n",
    "#     tmp = tensor.sub(mu).div(std + 0.00001)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_labels(target):\n",
    "    tmp = target.new_zeros(target.size(0), target.max() + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We normalized the data so it has mean 0 and std 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = normalize_data(train_input)\n",
    "test_input = normalize_data(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean = -1.1463554550061872e-08\n",
      "Training set std = 1.0\n",
      "\n",
      "Test set mean = 2.992591134898248e-07\n",
      "Test set std = 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set mean = {a}\".format(a = train_input.mean().item()))\n",
    "print(\"Training set std = {s}\\n\".format(s = train_input.std().item()))\n",
    "print(\"Test set mean = {a}\".format(a = test_input.mean().item()))\n",
    "print(\"Test set std = {s}\".format(s = test_input.std().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We will create a validation set to tune hyperparameters. This validation set is created from the training set in order to have fully independent testing data._\n",
    "\n",
    "_80% of the training data goes to training and the remaining 20% for our validation set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_permutation = torch.randperm(train_input.size(0))\n",
    "\n",
    "validation_proportion = 0.2\n",
    "split = int(0.2 * train_input.size(0))\n",
    "\n",
    "validation_index = index_permutation[:split]\n",
    "training_index = index_permutation[split:]\n",
    "\n",
    "validation_input = train_input[validation_index]\n",
    "validation_target = train_target[validation_index]\n",
    "validation_classes = train_classes[validation_index]\n",
    "\n",
    "train_input = train_input[training_index]\n",
    "train_target = train_target[training_index]\n",
    "train_classes = train_classes[training_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(train_input.size(0))\n",
    "print(validation_input.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating & Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, aux_loss = False):\n",
    "        \n",
    "        super(Net1, self).__init__()\n",
    "        self.aux_loss = aux_loss\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            self.fc1_aux = nn.Linear(1600, 128)\n",
    "            self.fc2_aux = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            x_aux = F.max_pool2d(x, 2)\n",
    "            x_aux = torch.flatten(x_aux, 1)\n",
    "            x_aux = self.fc1_aux(x_aux)\n",
    "            x_aux = F.relu(x_aux)\n",
    "            x_aux = self.fc2_aux(x_aux)\n",
    "            output_aux = F.softmax(x_aux, dim = 1)\n",
    "            \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            return output, output_aux\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, aux_loss = False):\n",
    "        \n",
    "        super(Net2, self).__init__()\n",
    "        self.aux_loss = aux_loss\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.fc1 = nn.Linear(2 * 1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            self.fc1_aux = nn.Linear(1600, 128)\n",
    "            self.fc2_aux = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            x_aux = F.max_pool2d(x, 2)\n",
    "            x_aux = torch.flatten(x_aux, 1)\n",
    "            x_aux = self.fc1_aux(x_aux)\n",
    "            x_aux = F.relu(x_aux)\n",
    "            x_aux = self.fc2_aux(x_aux)\n",
    "            output_aux = F.softmax(x_aux, dim = 1)\n",
    "            \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            return output, output_aux\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self, aux_loss = False):\n",
    "        \n",
    "        super(Net3, self).__init__()\n",
    "        self.aux_loss = aux_loss\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.fc1 = nn.Linear(2 * 576, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            self.fc1_aux = nn.Linear(800, 128)\n",
    "            self.fc2_aux = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            x_aux = F.max_pool2d(x, 2)\n",
    "            x_aux = torch.flatten(x_aux, 1)\n",
    "            x_aux = self.fc1_aux(x_aux)\n",
    "            x_aux = F.relu(x_aux)\n",
    "            x_aux = self.fc2_aux(x_aux)\n",
    "            output_aux = F.softmax(x_aux, dim = 1)\n",
    "            \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        \n",
    "        if self.aux_loss:\n",
    "            return output, output_aux\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_input, test_target, test_classes, model, criterion, batch_size, with_aux_loss = False):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        nb_data_errors = 0\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for inputs, targets in zip(test_input.split(batch_size),\n",
    "                                  test_target.split(batch_size)):\n",
    "            \n",
    "            if with_aux_loss:\n",
    "                outputs, output_aux = model(inputs)\n",
    "                aux_loss = criterion(output_aux, targets)\n",
    "                primary_loss = criterion(outputs, targets)\n",
    "                loss = primary_loss +  0.6 * aux_loss\n",
    "                \n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss_sum += loss\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "            \n",
    "            for k in range(len(inputs)):\n",
    "                if targets[k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "        accuracy = (1 - (nb_data_errors / test_input.size(0))) * 100\n",
    "        \n",
    "        return accuracy, loss_sum.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, test_input, test_target, test_classes, nb_epoch, batch_size, optimizer_params, logging = False, with_aux_loss = False):\n",
    "    \n",
    "    nb_epoch, batch_size = nb_epoch, batch_size\n",
    "    lr, momentum, weight_decay, gamma = optimizer_params['lr'], optimizer_params['momentum'], optimizer_params['weight_decay'], optimizer_params['gamma'] \n",
    "#     optimizer = torch.optim.Adam(model.parameters()) #, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if logging:\n",
    "        log_acc_loss_header(color=Color.GREEN)\n",
    "    \n",
    "        train_accuracies = []\n",
    "        train_losses = []\n",
    "        test_accuracies = []\n",
    "        test_losses = []\n",
    "        start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for e in range(nb_epoch):\n",
    "\n",
    "        for inputs, targets in zip(train_input.split(batch_size),\n",
    "                                  train_target.split(batch_size)):\n",
    "            \n",
    "            if with_aux_loss:\n",
    "                outputs, output_aux = model(inputs) \n",
    "                aux_loss = criterion(output_aux, targets)\n",
    "                primary_loss = criterion(outputs, targets)\n",
    "                loss = primary_loss + 0.6 * aux_loss\n",
    "                \n",
    "            else: \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "        \n",
    "        if logging:    \n",
    "            train_acc, train_loss = test(train_input, train_target, train_classes, model, criterion, batch_size, with_aux_loss)\n",
    "            test_acc, test_loss = test(test_input, test_target, test_classes, model, criterion, batch_size, with_aux_loss)\n",
    "        \n",
    "            train_accuracies.append(train_acc)\n",
    "            train_losses.append(train_loss)\n",
    "            test_accuracies.append(test_acc)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time\n",
    "            log_acc_loss(e, nb_epoch, elapsed_time, train_loss, train_acc, test_loss, test_acc, persistent=False)\n",
    "            \n",
    "    if logging:\n",
    "        print()\n",
    "        return train_accuracies, train_losses, test_accuracies, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Testing if the training is done correctly._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch       Time    Train loss     Train accuracy      Test loss      Test accuracy       \u001b[0m\n",
      "[100/100]   82s     6.8226         98.3750             12.4863        81.4000             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with_aux_loss = True\n",
    "basic_model = Net3(aux_loss = with_aux_loss)\n",
    "# basic_model = Baseline_Aux_Network()\n",
    "basic_model = basic_model.to(device=device)\n",
    "\n",
    "train_accuracies, train_losses, test_accuracies, test_losses = train_model(basic_model,\n",
    "                                                                           train_input, \n",
    "                                                                           train_target, \n",
    "                                                                           train_classes, \n",
    "                                                                           test_input, \n",
    "                                                                           test_target, \n",
    "                                                                           test_classes, \n",
    "                                                                           100, \n",
    "                                                                           BATCH_SIZE, \n",
    "                                                                           {'lr': 0.1, 'momentum':0.9, 'weight_decay': 0.0, 'gamma': 0.99}, \n",
    "                                                                           logging = True,\n",
    "                                                                           with_aux_loss = with_aux_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We will now tune hyperparameters. For now, we are tuning the learning rate, the momemtum and the number of epochs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "momentums = [0.9] #[0.5, 0.7, 0.9]\n",
    "nb_epochs = [20]#, 50, 100] #, 100]\n",
    "weight_decays = [0.0, 0.01, 0.1]\n",
    "gamma = 0.8\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentums:\n",
    "        for nb_epoch in nb_epochs:\n",
    "            for weight_decay in weight_decays:\n",
    "            \n",
    "                # creating params for optimizer\n",
    "                optimizer_params = {'lr':lr, 'momentum':momentum, 'weight_decay': weight_decay, 'gamma': gamma}\n",
    "                \n",
    "                # initialize raw model\n",
    "                model = models.BaselineNetwork()\n",
    "                model = model.to(device=device)\n",
    "                \n",
    "                # train model on training data\n",
    "                train_model(model,\n",
    "                            train_input,\n",
    "                            train_target,\n",
    "                            train_classes,\n",
    "                            None,\n",
    "                            None,\n",
    "                            None,\n",
    "                            nb_epoch, BATCH_SIZE, optimizer_params)\n",
    "                \n",
    "                # compute accuracy on validation data\n",
    "                accuracy, loss = test(validation_input, validation_target, validation_classes, model, nn.CrossEntropyLoss(), BATCH_SIZE)\n",
    "                \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params['lr'] = lr\n",
    "                    best_params['momentum'] = momentum\n",
    "                    best_params['nb_epoch'] = nb_epoch\n",
    "                    best_params['weight_decay'] = weight_decay\n",
    "                    \n",
    "print(\"Best accuracy obtained = {a}\\n\".format(a = best_accuracy))\n",
    "print(\"with the following hyperparameters:\\n\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Hyperparameters tuning is not optimal yet, do not consider the above results as  good yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer_params = {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0, 'gamma': 0.99}\n",
    "best_nb_epoch = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now that we have the best hyperparameters let's retrained the model and visualize the evolution of accuracy and loss on both the train and test sets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(train_accuracies, train_losses, test_accuracies, test_losses):\n",
    "    n = len(train_accuracies)\n",
    "    major_ticks = list(range(0, n, 10))\n",
    "    minor_ticks = list(range(0, n, 1))\n",
    "    \n",
    "    fig, axs = plt.subplots(2, dpi=240, figsize=(15, 12))\n",
    "    axs[0].plot(train_accuracies, color='Blue')\n",
    "    axs[0].plot(test_accuracies, color='Red')\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set(xlabel='Training epochs', ylabel='Accuracy (%)')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(['Train set', 'Test set'])\n",
    "    axs[0].set_xlim(left=0)\n",
    "    axs[0].set_xlim(right=n-1)\n",
    "    axs[0].set_xticks(major_ticks)\n",
    "    axs[0].set_xticks(minor_ticks, minor=True)\n",
    "    axs[0].grid(which='minor', alpha=0.3)\n",
    "    axs[0].grid(which='major', alpha=0.7)\n",
    "    \n",
    "    axs[1].plot(train_losses, color='Blue')\n",
    "    axs[1].plot(test_losses, color='Red')\n",
    "    axs[1].set_title(\"Cross-Entropy Loss\")\n",
    "    axs[1].set(xlabel='Training epochs', ylabel='Loss')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(['Train set', 'Test set'])\n",
    "    axs[1].set_xticks(major_ticks)\n",
    "    axs[1].set_xticks(minor_ticks, minor=True)\n",
    "    axs[1].grid(which='minor', alpha=0.3)\n",
    "    axs[1].grid(which='major', alpha=0.7)\n",
    "    axs[1].set_xlim(left=0)\n",
    "    axs[1].set_xlim(right=n-1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Net()\n",
    "final_model = final_model.to(device)\n",
    "# We regenerate data to train the model on all the training data available. \n",
    "# We don't need the validation split at this step.\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_data_device(1000, device=device)\n",
    "train_input = normalize_data(train_input)\n",
    "test_input = normalize_data(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch       Time    Train loss     Train accuracy      Test loss      Test accuracy       \u001b[0m\n",
      "[150/150]   70s     0.0002         100.0000            22.4878        81.7000             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_accuracies, train_losses, test_accuracies, test_losses = train_model(final_model,\n",
    "                                                                           train_input, \n",
    "                                                                           train_target, \n",
    "                                                                           train_classes, \n",
    "                                                                           test_input, \n",
    "                                                                           test_target, \n",
    "                                                                           test_classes, \n",
    "                                                                           best_nb_epoch, \n",
    "                                                                           BATCH_SIZE, \n",
    "                                                                           best_optimizer_params, \n",
    "                                                                           logging = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(train_accuracies, train_losses, test_accuracies, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In order to test the model we will generate new data (training and test set), retrained the model on the new data en evaluate it on the new test set. We will do this process more than 10 times and estimates the mean accuracy as well as its standard deviation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rounds = 10\n",
    "test_model = models.BaselineNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, nb_rounds, criterion):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for round in range(nb_rounds):\n",
    "        \n",
    "        # initialize new model\n",
    "        model_evaluated = model().to(device)\n",
    "        # generate new data\n",
    "        train_input, train_target, train_classes, test_input, test_target, test_classes = generate_data_device(1000, device=device)\n",
    "        train_input = normalize_data(train_input)\n",
    "        test_input = normalize_data(test_input)\n",
    "        \n",
    "        train_model(model_evaluated,\n",
    "                    train_input,\n",
    "                    train_target,\n",
    "                    train_classes,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    best_nb_epoch, BATCH_SIZE, best_optimizer_params)\n",
    "        \n",
    "        accuracy, loss = test(test_input, test_target, test_classes, model_evaluated, criterion, BATCH_SIZE)\n",
    "        \n",
    "        print(\"Round {i}: accuracy = {a:0.2f}% | loss = {l:0.4f}\".format(i = (round + 1), a = accuracy, l = loss))\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return torch.FloatTensor(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = evaluate_model(test_model, nb_rounds, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean accuracy is: {a:0.2f}\".format(a = accuracies.mean()))\n",
    "print(\"The accuracy std is: {s:0.4f}\".format(s = accuracies.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
