{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dlc_practical_prologue import generate_pair_sets\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate raw data, process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(tensor):\n",
    "    mu, std = tensor.mean(), tensor.std()\n",
    "    tmp = tensor.sub(mu).div(std)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_labels(target):\n",
    "    tmp = target.new_zeros(target.size(0), target.max() + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We normalized the data so it has mean 0 and std 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = normalize_data(train_input)\n",
    "\n",
    "\n",
    "test_input = normalize_data(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean = -1.1463554550061872e-08\n",
      "Training set std = 1.0\n",
      "\n",
      "Test set mean = 2.992591134898248e-07\n",
      "Test set std = 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set mean = {a}\".format(a = train_input.mean().item()))\n",
    "print(\"Training set std = {s}\\n\".format(s = train_input.std().item()))\n",
    "print(\"Test set mean = {a}\".format(a = test_input.mean().item()))\n",
    "print(\"Test set std = {s}\".format(s = test_input.std().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We will create a validation set to tune hyperparameters. This validation set is created from the training set in order to have fully independent testing data._\n",
    "\n",
    "_80% of the training data goes to training and the remaining 20% for our validation set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_permutation = torch.randperm(train_input.size(0))\n",
    "\n",
    "validation_proportion = 0.2\n",
    "split = int(0.2 * train_input.size(0))\n",
    "\n",
    "validation_index = index_permutation[:split]\n",
    "training_index = index_permutation[split:]\n",
    "\n",
    "validation_input = train_input[validation_index]\n",
    "validation_target = train_target[validation_index]\n",
    "validation_classes = train_classes[validation_index]\n",
    "\n",
    "train_input = train_input[training_index]\n",
    "train_target = train_target[training_index]\n",
    "train_classes = train_classes[training_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating & Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = models.BaselineNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, nb_epoch, batch_size, optimizer_params):\n",
    "    nb_epoch, batch_size = nb_epoch, batch_size\n",
    "    lr, momentum = optimizer_params['lr'], optimizer_params['momentum']\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for e in range(nb_epoch):\n",
    "        for inputs, targets in zip(train_input.split(batch_size),\n",
    "                                  train_target.split(batch_size)):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(basic_model, train_input, train_target, train_classes, 50, BATCH_SIZE, {'lr': 0.01, 'momentum':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_input, test_target, test_classes, model, criterion):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        nb_data_errors = 0\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for inputs, targets in zip(test_input.split(BATCH_SIZE),\n",
    "                                  test_target.split(BATCH_SIZE)):\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "            \n",
    "            for k in range(len(inputs)):\n",
    "                if targets[k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "        accuracy = (1 - (nb_data_errors / test_input.size(0))) * 100\n",
    "\n",
    "        \n",
    "        return accuracy, loss_sum.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.9, 8.051664352416992)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_input, test_target, test_classes, basic_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy obtained = 76.5\n",
      "\n",
      "with the following hyperparameters:\n",
      "\n",
      "{'lr': 0.01, 'momentum': 0.9, 'nb_epoch': 50}\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01] #, 0.1]\n",
    "momentums = [0.9] #[0.5, 0.7, 0.9]\n",
    "nb_epochs = [20, 50] #, 100]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentums:\n",
    "        for nb_epoch in nb_epochs:\n",
    "            \n",
    "                # creating params for optimizer\n",
    "                optimizer_params = {'lr':lr, 'momentum':momentum}\n",
    "                \n",
    "                # initialize raw model\n",
    "                model = models.BaselineNetwork()\n",
    "                \n",
    "                # train model on training data\n",
    "                train_model(model, train_input, train_target, train_classes, nb_epoch, BATCH_SIZE, optimizer_params)\n",
    "                \n",
    "                # compute accuracy on validation data\n",
    "                accuracy, loss = test(validation_input, validation_target, validation_classes, model, nn.CrossEntropyLoss())\n",
    "                            \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params['lr'] = lr\n",
    "                    best_params['momentum'] = momentum\n",
    "                    best_params['nb_epoch'] = nb_epoch\n",
    "                    \n",
    "print(\"Best accuracy obtained = {a}\\n\".format(a = best_accuracy))\n",
    "print(\"with the following hyperparameters:\\n\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In order to test the model we will generate new data (training and test set), retrained the model on the new data en evaluate it on the new test set. We will do this process more than 10 times and estimates the mean accuracy as well as its standard deviation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer_params = {'lr': 0.01, 'momentum': 0.9}\n",
    "best_nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rounds = 10\n",
    "test_model = models.BaselineNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, nb_rounds, criterion):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for round in range(nb_rounds):\n",
    "        \n",
    "        # initialize new model\n",
    "        model_evaluated = model()\n",
    "        # generate new data\n",
    "        train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)\n",
    "        train_input = normalize_data(train_input)\n",
    "        test_input = normalize_data(test_input)\n",
    "        \n",
    "        train_model(model_evaluated, train_input, train_target, train_classes, best_nb_epoch, BATCH_SIZE, best_optimizer_params)\n",
    "        \n",
    "        accuracy, loss = test(test_input, test_target, test_classes, model_evaluated, criterion)\n",
    "        \n",
    "        print(\"Round {i}: accuracy = {a}% | loss = {l}\".format(i = (round + 1), a = accuracy, l = loss))\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return torch.FloatTensor(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: accuracy = 80.3% | loss = 9.005359649658203\n",
      "Round 2: accuracy = 80.89999999999999% | loss = 7.677920341491699\n",
      "Round 3: accuracy = 79.80000000000001% | loss = 9.256414413452148\n",
      "Round 4: accuracy = 77.8% | loss = 9.148301124572754\n",
      "Round 5: accuracy = 80.1% | loss = 8.6519136428833\n",
      "Round 6: accuracy = 78.10000000000001% | loss = 8.843724250793457\n",
      "Round 7: accuracy = 78.0% | loss = 9.749249458312988\n",
      "Round 8: accuracy = 81.1% | loss = 7.922359466552734\n",
      "Round 9: accuracy = 80.3% | loss = 7.184577941894531\n",
      "Round 10: accuracy = 82.2% | loss = 8.34280776977539\n"
     ]
    }
   ],
   "source": [
    "accuracies = evaluate_model(test_model, nb_rounds, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy is: 79.86000061035156\n",
      "The accuracy std is: 1.4660598039627075\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy is: {a}\".format(a = accuracies.mean()))\n",
    "print(\"The accuracy std is: {s}\".format(s = accuracies.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
