{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dlc_practical_prologue import generate_pair_sets\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate raw data, process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_0, train_target_0, train_classes_0, test_input_0, test_target_0, test_classes_0 = generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(tensor):\n",
    "    mu, std = tensor.mean(), tensor.std()\n",
    "    tmp = tensor.sub(mu).div(std)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_labels(target):\n",
    "    tmp = target.new_zeros(target.size(0), target.max() + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We normalized the data so it has mean 0 and std 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = normalize_data(train_input_0)\n",
    "# train_target = convert_to_one_hot_labels(train_target_0)\n",
    "train_target = train_target_0\n",
    "\n",
    "test_input = normalize_data(test_input_0)\n",
    "# test_target = convert_to_one_hot_labels(test_target_0)\n",
    "test_target = test_target_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean = -1.1463554550061872e-08\n",
      "Training set std = 1.0\n",
      "\n",
      "Test set mean = 2.992591134898248e-07\n",
      "Test set std = 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set mean = {a}\".format(a = train_input.mean().item()))\n",
    "print(\"Training set std = {s}\\n\".format(s = train_input.std().item()))\n",
    "print(\"Test set mean = {a}\".format(a = test_input.mean().item()))\n",
    "print(\"Test set std = {s}\".format(s = test_input.std().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating & Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = models.BaselineNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, nb_epoch, batch_size, optimizer_params):\n",
    "    nb_epoch, batch_size = nb_epoch, batch_size\n",
    "    lr, momentum = optimizer_params['lr'], optimizer_params['momentum']\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for e in range(nb_epoch):\n",
    "        for inputs, targets in zip(train_input.split(batch_size),\n",
    "                                  train_target.split(batch_size)):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(basic_model, train_input, train_target, train_classes_0, 50, 32, {'lr': 0.01, 'momentum':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "LOG_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_input, test_target, test_classes, model, criterion):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        nb_data_errors = 0\n",
    "        loss_sum = 0\n",
    "        for b in range(0, test_input.size(0), BATCH_SIZE):\n",
    "            output = model(test_input.narrow(0, b, BATCH_SIZE))\n",
    "            loss = criterion(output, test_target.narrow(0, b, BATCH_SIZE))\n",
    "            loss_sum += loss\n",
    "            _, predicted_classes = torch.max(output, 1)\n",
    "\n",
    "            for k in range(BATCH_SIZE):\n",
    "                #print(repr(test_target[b + k]) + \" - \" + repr(predicted_classes[k]))\n",
    "                if test_target[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "            if b % LOG_INTERVAL == 0:\n",
    "                pass\n",
    "                #print(\"Accuracy: \" + repr())\n",
    "                #print_test(batch_idx, len(val_loader), batch_time, losses, top1, persistent=False, color=color, title=title)\n",
    "    \n",
    "#         print(\"Nb errors: \" + repr(nb_data_errors))\n",
    "        accuracy = 1 - (nb_data_errors / test_input.size(0))\n",
    "#         print(\"Accuracy: \" + repr(accuracy * 100) + \"%\" + \" - Loss: \" + repr(loss_sum.item()))\n",
    "        \n",
    "        return (accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.4"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_input, test_target, test_classes_0, basic_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.9\n",
      "20\n",
      "16\n",
      "\n",
      "\n",
      "0.001\n",
      "0.9\n",
      "20\n",
      "32\n",
      "\n",
      "\n",
      "0.001\n",
      "0.9\n",
      "50\n",
      "16\n",
      "\n",
      "\n",
      "0.001\n",
      "0.9\n",
      "50\n",
      "32\n",
      "\n",
      "\n",
      "0.01\n",
      "0.9\n",
      "20\n",
      "16\n",
      "\n",
      "\n",
      "0.01\n",
      "0.9\n",
      "20\n",
      "32\n",
      "\n",
      "\n",
      "0.01\n",
      "0.9\n",
      "50\n",
      "16\n",
      "\n",
      "\n",
      "0.01\n",
      "0.9\n",
      "50\n",
      "32\n",
      "\n",
      "\n",
      "Best accuracy obtained = 80.89999999999999\n",
      "\n",
      "with the following hyperparameters:\n",
      "\n",
      "{'lr': 0.001, 'momentum': 0.9, 'nb_epoch': 50, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01] #, 0.1]\n",
    "momentums = [0.9] #[0.5, 0.7, 0.9]\n",
    "nb_epochs = [20, 50] #, 100]\n",
    "batch_sizes = [16, 32] #, 64]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentums:\n",
    "        for nb_epoch in nb_epochs:\n",
    "            for batch_size in batch_sizes:\n",
    "                optimizer_params = {'lr':lr, 'momentum':momentum}\n",
    "                \n",
    "                model = models.BaselineNetwork()\n",
    "                \n",
    "                train_model(model, train_input, train_target, train_classes_0, nb_epoch, batch_size, optimizer_params)\n",
    "                \n",
    "                accuracy = test(test_input, test_target, test_classes_0, model, nn.CrossEntropyLoss())\n",
    "                \n",
    "                print(lr)\n",
    "                print(momentum)\n",
    "                print(nb_epoch)\n",
    "                print(batch_size)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params['lr'] = lr\n",
    "                    best_params['momentum'] = momentum\n",
    "                    best_params['nb_epoch'] = nb_epoch\n",
    "                    best_params['batch_size'] = batch_size\n",
    "                    \n",
    "print(\"Best accuracy obtained = {a}\\n\".format(a = best_accuracy))\n",
    "print(\"with the following hyperparameters:\\n\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In order to test the model we will generate new data (training and test set), retrained the model on the new data en evaluate it on the new test set. We will do this process more than 10 times and estimates the mean accuracy as well as its standard deviation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rounds = 10\n",
    "test_model = models.BaselineNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, nb_rounds, criterion):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for round in range(nb_rounds):\n",
    "        \n",
    "        # initialize new model\n",
    "        model_evaluated = model()\n",
    "        # generate new data\n",
    "        train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)\n",
    "        train_input = normalize_data(train_input)\n",
    "        test_input = normalize_data(test_input)\n",
    "        \n",
    "        train_model(model_evaluated, train_input, train_target, train_classes)\n",
    "        \n",
    "        accuracy = test(test_input, test_target, test_classes, model_evaluated, criterion)\n",
    "        \n",
    "        print(\"Round {i}: accuracy = {a}\".format(i = (round + 1), a = accuracy))\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return torch.FloatTensor(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: accuracy = 79.60000000000001\n",
      "Round 2: accuracy = 80.0\n",
      "Round 3: accuracy = 77.7\n",
      "Round 4: accuracy = 79.60000000000001\n",
      "Round 5: accuracy = 79.5\n",
      "Round 6: accuracy = 74.9\n",
      "Round 7: accuracy = 80.89999999999999\n",
      "Round 8: accuracy = 79.5\n",
      "Round 9: accuracy = 80.1\n",
      "Round 10: accuracy = 80.0\n"
     ]
    }
   ],
   "source": [
    "accuracies = evaluate_model(test_model, nb_rounds, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy is: 79.18000030517578\n",
      "The accuracy std is: 1.7067185640335083\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy is: {a}\".format(a = accuracies.mean()))\n",
    "print(\"The accuracy std is: {s}\".format(s = accuracies.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
