{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dlc_practical_prologue import generate_pair_sets\n",
    "import torch.nn as nn\n",
    "from print_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate raw data, process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_0, train_target_0, train_classes_0, test_input_0, test_target_0, test_classes_0 = generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(tensor):\n",
    "    mu, std = tensor.mean(), tensor.std()\n",
    "    tmp = tensor.sub(mu).div(std)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_labels(target):\n",
    "    tmp = target.new_zeros(target.size(0), target.max() + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We normalized the data so it has mean 0 and std 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = normalize_data(train_input_0)\n",
    "# train_target = convert_to_one_hot_labels(train_target_0)\n",
    "train_target = train_target_0\n",
    "\n",
    "test_input = normalize_data(test_input_0)\n",
    "# test_target = convert_to_one_hot_labels(test_target_0)\n",
    "test_target = test_target_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean = -4.7528015301168125e-08\n",
      "Training set std = 1.0\n",
      "\n",
      "Test set mean = 5.0797755335452166e-08\n",
      "Test set std = 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set mean = {a}\".format(a = train_input.mean().item()))\n",
    "print(\"Training set std = {s}\\n\".format(s = train_input.std().item()))\n",
    "print(\"Test set mean = {a}\".format(a = test_input.mean().item()))\n",
    "print(\"Test set std = {s}\".format(s = test_input.std().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating & Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = models.BaselineNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_input, test_target, test_classes, model, criterion, batch_size):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        nb_data_errors = 0\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for inputs, targets in zip(test_input.split(batch_size),\n",
    "                                  test_target.split(batch_size)):\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "            \n",
    "            for k in range(len(inputs)):\n",
    "                if targets[k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "        accuracy = (1 - (nb_data_errors / test_input.size(0))) * 100\n",
    "        \n",
    "        return accuracy, loss_sum.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def log_acc(epoch, max_epoch, test_acc, train_acc):\n",
    "#    print('[' + repr(epoch) + '/' + repr(max_epoch) + ']'.ljust(20) + repr(train_acc).ljust(20) + repr(test_acc).ljust(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, test_input, test_target, test_classes, nb_epoch, batch_size, optimizer_params):\n",
    "    nb_epoch, batch_size = nb_epoch, batch_size\n",
    "    lr, momentum = optimizer_params['lr'], optimizer_params['momentum']\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    log_acc_loss_header(color=Color.GREEN)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for e in range(nb_epoch):\n",
    "        for inputs, targets in zip(train_input.split(batch_size),\n",
    "                                  train_target.split(batch_size)):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        test_acc, test_loss = test(test_input, test_target, test_classes, model, criterion, batch_size)\n",
    "        train_acc, train_loss = test(train_input, train_target, train_classes, model, criterion, batch_size)\n",
    "        log_acc_loss(e, nb_epoch, 0, train_loss, train_acc, test_loss, test_acc, persistent=False)\n",
    "    log_acc_loss(e, nb_epoch, 0, train_loss, train_acc, test_loss, test_acc, persistent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch       Time    Train loss     Train accuracy      Test loss      Test accuracy       \u001b[0m\n",
      "[50/50]     0s      0.0022         100.0000            36.6962        80.9000             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_model(basic_model, train_input, train_target, train_classes_0, test_input, test_target, test_classes_0, 50, 32, {'lr': 0.01, 'momentum':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "LOG_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81.0, 31.63132667541504)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_input, test_target, test_classes_0, basic_model, nn.CrossEntropyLoss(), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01] #, 0.1]\n",
    "momentums = [0.9] #[0.5, 0.7, 0.9]\n",
    "nb_epochs = [20, 50] #, 100]\n",
    "batch_sizes = [16, 32] #, 64]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentums:\n",
    "        for nb_epoch in nb_epochs:\n",
    "            for batch_size in batch_sizes:\n",
    "                optimizer_params = {'lr':lr, 'momentum':momentum}\n",
    "                \n",
    "                model = models.BaselineNetwork()\n",
    "                \n",
    "                train_model(model, train_input, train_target, train_classes_0, nb_epoch, batch_size, optimizer_params)\n",
    "                \n",
    "                accuracy = test(test_input, test_target, test_classes_0, model, nn.CrossEntropyLoss())\n",
    "                \n",
    "                print(lr)\n",
    "                print(momentum)\n",
    "                print(nb_epoch)\n",
    "                print(batch_size)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params['lr'] = lr\n",
    "                    best_params['momentum'] = momentum\n",
    "                    best_params['nb_epoch'] = nb_epoch\n",
    "                    best_params['batch_size'] = batch_size\n",
    "                    \n",
    "print(\"Best accuracy obtained = {a}\\n\".format(a = best_accuracy))\n",
    "print(\"with the following hyperparameters:\\n\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In order to test the model we will generate new data (training and test set), retrained the model on the new data en evaluate it on the new test set. We will do this process more than 10 times and estimates the mean accuracy as well as its standard deviation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rounds = 10\n",
    "test_model = models.BaselineNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, nb_rounds, criterion):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for round in range(nb_rounds):\n",
    "        \n",
    "        # initialize new model\n",
    "        model_evaluated = model()\n",
    "        # generate new data\n",
    "        train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)\n",
    "        train_input = normalize_data(train_input)\n",
    "        test_input = normalize_data(test_input)\n",
    "        \n",
    "        train_model(model_evaluated, train_input, train_target, train_classes, 50, 16, {'lr': 0.01, 'momentum': 0.9})\n",
    "        \n",
    "        accuracy = test(test_input, test_target, test_classes, model_evaluated, criterion)\n",
    "        \n",
    "        print(\"Round {i}: accuracy = {a}\".format(i = (round + 1), a = accuracy))\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return torch.FloatTensor(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = evaluate_model(test_model, nb_rounds, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean accuracy is: {a}\".format(a = accuracies.mean()))\n",
    "print(\"The accuracy std is: {s}\".format(s = accuracies.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
