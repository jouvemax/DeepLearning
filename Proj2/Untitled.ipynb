{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fcb6850f390>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import empty\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object) :\n",
    "    \n",
    "    def forward (self , *input) :\n",
    "        raise NotImplementedError\n",
    "    def backward (self , *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        return\n",
    "        \n",
    "    def forward(self, input) :\n",
    "        self.input = input\n",
    "        return input.clamp(min=0)\n",
    "    def backward(self, grad_output):\n",
    "        assert(self.input is not None)\n",
    "        assert(grad_output.size == self.input.size)\n",
    "        \n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[self.input < 0] = 0\n",
    "        return grad_input\n",
    "        \n",
    "    __call__ = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.tanh(input)\n",
    "    def backward (self, *gradwrtoutput):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the mean square error loss between \n",
    "        the input tensor and the target tensor.\n",
    "\n",
    "        Args:\n",
    "        input -- tensor of size (N, *)\n",
    "        target -- tensor of size (N, *)\n",
    "\n",
    "        Returns:\n",
    "        loss -- mse loss between input and target, loss = \n",
    "        \"\"\"\n",
    "    \n",
    "        assert(input.size() == target.size())\n",
    "\n",
    "        N = input.size(0)\n",
    "        loss = (target-input).pow(2).sum()\n",
    "        loss = 1/(2*N) * loss\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, input, target):\n",
    "        assert(input.size() == target.size())\n",
    "        N = input.size(0)\n",
    "        return 1/N * (input-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "   \n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        mean = 0\n",
    "        std = 1\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.params = []\n",
    "        \n",
    "        self.weight = torch.empty(size=(out_features, in_features)).normal_(mean=mean, std=std)\n",
    "        self.dw = torch.zeros(size=(out_features, in_features))\n",
    "        self.params.append((self.weight, self.dw))\n",
    "        if bias:\n",
    "            self.bias = torch.empty(out_features).normal_(mean=mean, std=std)\n",
    "            self.db = torch.zeros(out_features)\n",
    "            self.params.append((self.bias, self.db))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.db = None\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward the input data by applying a linear transformation on it.\n",
    "        \n",
    "        Args:\n",
    "        input -- tensor of size (N, *, in_features)\n",
    "        \n",
    "        Returns:\n",
    "        output -- tensor of size (N, *, out_features),  output = input @ weight.T + bias\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(input.size(-1) == self.in_features)\n",
    "        \n",
    "        self.input = input.clone() # Required information for the backward pass.\n",
    "        \n",
    "        output = input @ self.weight.T\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Compute the derivative w.r.t. the input of the layer\n",
    "        given the derivate w.r.t. to the output of the layer.\n",
    "        \n",
    "        Args:\n",
    "        grad_output -- tensor of size (N, *, out_features)\n",
    "        \n",
    "        Returns \n",
    "        grad_input -- tensor of size (N, * , in_features), grad_input = grad_output @ weight\n",
    "        \"\"\"\n",
    "\n",
    "        assert(grad_output.size(-1) == self.out_features)\n",
    "        \n",
    "        grad_input = grad_output @ self.weight\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            self.db += grad_output.sum(axis=0)\n",
    "        self.dw += grad_output.T @  self.input\n",
    "        \n",
    "        return grad_input\n",
    "       \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient w.r.t. the parametes to zero i.e.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dw = torch.zeros(size=(self.out_features, self.in_features))\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            self.db = torch.zeros(self.out_features)\n",
    "        return\n",
    "    \n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        params -- a list of pairs, each composed of a parameter tensor, \n",
    "        and a gradient tensor of same size.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We just return a copy as we don't want the user\n",
    "        # to be able to change the params of the model through this method.\n",
    "        params = self.params.clone() \n",
    "        return params \n",
    "    \n",
    "\n",
    "    def update_params(self, step_size):\n",
    "        \"\"\"\n",
    "        Update the parameters of the linear layer going \n",
    "        in the opposite direction of the gradient.\n",
    "        \n",
    "        Args:\n",
    "        step_size -- the size of an update step\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weight -= step_size * self.dw\n",
    "        if self.bias is not None:\n",
    "            self.bias -= step_size * self.db\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-161-6d202781e7a6>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-161-6d202781e7a6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Sequential(Module):\n",
    "    \n",
    "    def __init__(self, *modules):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Little example with 1 linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6002,  0.4820],\n",
      "        [-0.0446,  1.4544],\n",
      "        [-0.2405, -0.0255]])\n",
      "tensor([ 0.1009, -1.1129,  0.3157])\n"
     ]
    }
   ],
   "source": [
    "linearLayer = Linear(2,3, bias=True)\n",
    "loss_module = LossMSE()\n",
    "\n",
    "#generate data\n",
    "x = torch.empty(10,2).normal_()\n",
    "real_w = torch.empty(3,2).normal_()\n",
    "real_b = torch.empty(3).normal_()\n",
    "y = x @ real_w.T + real_b\n",
    "print(real_w)\n",
    "print(real_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearLayer.param()[0][0][0] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.0000, 10.0000],\n",
       "        [-0.9219, -0.3727],\n",
       "        [ 1.2722, -0.0885]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearLayer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n",
      "weight: tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "bias: tensor([0.6385, 1.8439, 0.5255])\n",
      "loss: 1.429478822638497e-12\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "epoch = 1000\n",
    "for e in range(epoch):\n",
    "    output = linearLayer.forward(x)\n",
    "    loss = loss_module.forward(output, y)\n",
    "    if e%100 == 0:\n",
    "        print(\"weight: {0}\".format(linearLayer.weight))\n",
    "        print(\"bias: {0}\".format(linearLayer.bias))\n",
    "        print(\"loss: {0}\".format(loss))\n",
    "    dloss = loss_module.backward(output, y)\n",
    "    linearLayer.backward(dloss)\n",
    "    linearLayer.update_params(0.1)\n",
    "    linearLayer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6695,  0.6480],\n",
      "        [-0.7930,  0.1627],\n",
      "        [-1.5000,  1.4641]])\n",
      "tensor([0.6385, 1.8439, 0.5255])\n"
     ]
    }
   ],
   "source": [
    "print(real_w)\n",
    "print(real_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
